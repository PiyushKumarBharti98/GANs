{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c24f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8c24f40",
    "outputId": "2799fc35-fce8-4ab3-e5f0-93b6cf632440"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import scipy\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython.display import clear_output\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92050e79",
   "metadata": {
    "id": "92050e79"
   },
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \n",
    "      def __init__(self, **kwargs):\n",
    "            self.__dict__.update(kwargs)\n",
    "\n",
    "hp = Hyperparameters(\n",
    "    epoch=0,\n",
    "    n_epochs=200,\n",
    "    batch_size=8,        \n",
    "    dataset_train_mode=\"train\",\n",
    "    dataset_test_mode=\"val\",    \n",
    "    lr=.0002,    \n",
    "    b1=.5,\n",
    "    b2=0.999,\n",
    "    n_cpu=8,\n",
    "    img_size=128,\n",
    "    channels=3,\n",
    "    latent_dim=8,\n",
    "    n_critic=5,\n",
    "    sample_interval=400,\n",
    "    lambda_pixel=10,\n",
    "    lambda_latent=.5,\n",
    "    lambda_kl=.01)\n",
    "img_root_folder = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GANs\\\\facades'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95543c17",
   "metadata": {
    "id": "95543c17"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,root,transforms_=None,mode='train'):\n",
    "        self.transforms = transforms.Compose(transforms_)\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "        if mode == \"train\":\n",
    "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        img = Image.open(self.files[index%len(self.files)]),\n",
    "        w,h=img.size\n",
    "        img_A = img.crop((0,0,w/2,h))\n",
    "        img_B = img.crop((w/2,0,w,h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            \n",
    "            img_A = Image.fromarray(np.array(img_A)[:,::-1,:],\"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:,::-1,:],\"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transfrom(img_B)\n",
    "\n",
    "        return {\"A\" : img_A, \"B\" : img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "          return len(self.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "RBWtLtNSVR55",
   "metadata": {
    "id": "RBWtLtNSVR55"
   },
   "outputs": [],
   "source": [
    "def imshow(img,size=10):\n",
    "    img = img/2+0.5\n",
    "    npimg = img.numpu()\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "def visualize_output(path,w,h):\n",
    "      img = mpimg.imread(path)\n",
    "      plt.figure(figsize=(w,h))\n",
    "      plt.imshow(img)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "JfPfUjK9We-n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "JfPfUjK9We-n",
    "outputId": "d839f20b-a7cb-4674-e37d-d3169bfb6d15"
   },
   "outputs": [],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((hp.img_size, hp.img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(img_root_folder, mode=hp.dataset_train_mode, transforms_=transforms_),  # Fix here\n",
    "    batch_size=hp.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(img_root_folder, mode=hp.dataset_test_mode, transforms_=transforms_),  # Fix here\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "NUzquCqgwY-z",
   "metadata": {
    "id": "NUzquCqgwY-z"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d9b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_class_init(m):\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\")!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,0.0,0.2)\n",
    "    if classname.find(\"BatchNorm2d\")!=-1:\n",
    "        torch.nn.init.normal_(m.weight.data,1.0,0.2)\n",
    "        torch.nn.init.constant_(m.weight.data,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca5677aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "     def __init__(self,in_size,out_size,normalize=True,dropout=0.0):\n",
    "            super(UNetDown,self).__init__()\n",
    "            layers = [nn.Conv2d(in_size,out_size,3,stride=2,padding=1,bias=False)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_size,0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            self.model= nn.Sequential(*layers)\n",
    "     \n",
    "     def forward(self,x):\n",
    "        return self.model(x)       \n",
    "    \n",
    "    \n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self,in_size,out_size):\n",
    "        super(UNetUp,self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "                   nn.Upsample(scale_factor=2),\n",
    "                   nn.Conv2d(in_size,out_size,3,stride=2,padding=1,bias=False),\n",
    "                   nn.BatchNorm2d(out_size,0.8),\n",
    "                   nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x,skip_input):\n",
    "        x=self.model(x),\n",
    "        x=torch.cat((x,skip_input),1)\n",
    "        return x\n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62bfbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,latent_dim,img_shape):\n",
    "        super(Generator,self).__init__()\n",
    "        channels,self.h,self.w=img_shape\n",
    "        \n",
    "        self.fc = nn.Linear(latent_dim,self.h*self.w)\n",
    "        \n",
    "        self.down1=UNetDown(channels+1,64,normalize=True)\n",
    "        self.down2=UNetDown(64,128)\n",
    "        self.down3=UNetDown(128,256)\n",
    "        self.down4=UNetDown(256,512)\n",
    "        self.down5=UNetDown(512,512)\n",
    "        self.down6=UNetDown(512,512)\n",
    "        self.down7=UNetDown(512,512,normalize=False)\n",
    "        self.up1=UNetUp(512,512)\n",
    "        self.up2=UNetUp(1024,512)\n",
    "        self.up3=UNetUp(1024,512)\n",
    "        self.up4=UNetUp(1024,256)\n",
    "        self.up5=UNetUp(215,128)\n",
    "        self.up6=UNetUp(256,64)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "                     nn.Upsample(scale_factor=2),\n",
    "                     nn.Conv2d(128,channels,3,stride=1,padding=1),nn.Tanh())\n",
    "        \n",
    "        def forward(self,x,z):\n",
    "            z = self.fc(z).view(z.size(0),1,self.h,self.w)\n",
    "            d1=self.down1(torch.cat(x,z),1)\n",
    "            d2=self.down2(d1)\n",
    "            d3=self.down3(d2)\n",
    "            d4=self.down2(d3)\n",
    "            d5=self.down2(d4)\n",
    "            d6=self.down2(d5)\n",
    "            d7=self.down7(d6)\n",
    "            u1=self.up1(d7,d6)\n",
    "            u2=self.up2(u1,d5)\n",
    "            u3=self.up3(u2,d4)\n",
    "            u4=self.up4(u3,d3)\n",
    "            u5=self.up5(u4,d2)\n",
    "            u6=self.up6(u5,d1)\n",
    "            \n",
    "            return self.final(u6)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5821290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,latent_dim,input_shape):\n",
    "        super(Encoder,self).__init__()\n",
    "        resnet18_model = resnet18(pretrained=False)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
    "        self.pooling = nn.AvgPool2d(kernel_size = 8, stride=8, padding=0)\n",
    "        self.fc_mu = nn.Linear(256,latent_dim)\n",
    "        self.fc_logvar = nn.Linear(25,latent_dim)\n",
    "        \n",
    "    def forward(self,img):\n",
    "        out = self.feature_extractor(img)\n",
    "        out = self.poolin(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        mu = self.fc_mu(out)\n",
    "        var = self.fc_logvar(out)\n",
    "        return mu,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac922a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDiscriminator(nn.Module):\n",
    "        def __init__(self,input_shape):\n",
    "            super(MultiDiscriminator,self).__init__()\n",
    "            def discriminator_block(in_filters,out_filters,normalize=True):\n",
    "                    layers = [nn.Conv2d(in_filters,out_filters,4,stride=2,padding=1)]\n",
    "                    if normalize:\n",
    "                        layers.append(nn.BatchNorm2d(out_filters,0.8))\n",
    "                    layers.append(nn.LeakyReLU(0.8))\n",
    "                    return layers\n",
    "            \n",
    "            channels,_,_=input_shape\n",
    "            self.models = nn.ModuleList()\n",
    "            for i in range(3):\n",
    "                self.models.add_module(\n",
    "                    \"disc_%d\"%i,\n",
    "                    nn.Sequential(\n",
    "                    *discriminator_block(channels,64,normalize=False),\n",
    "                    *discriminator_block(64,128),\n",
    "                    *discriminator_block(128,264),\n",
    "                    *discriminator_block(264,512),\n",
    "                    nn.Conv2d(512,1,3,padding=2)\n",
    "                    ),\n",
    "                )\n",
    "            self.downsampling = nn.AvgPool2d(channels,stride=1,padding=[1,1],count_include_pad=False)\n",
    "            \n",
    "            def compute_loss(self,x,ground_truth):\n",
    "                loss = sum([torch.mean((out-ground_truth)**2) for out in outputs])\n",
    "                return loss\n",
    "            \n",
    "            def forward(self,x):\n",
    "                outputs=[]\n",
    "                for m in models:\n",
    "                    outputs.append(m(x))\n",
    "                    x=self.downsample(x)\n",
    "                    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8542fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using CUDA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiDiscriminator(\n",
       "  (models): ModuleList(\n",
       "    (0-2): 3 x Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.8)\n",
       "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): LeakyReLU(negative_slope=0.8)\n",
       "      (5): Conv2d(128, 264, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (6): BatchNorm2d(264, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.8)\n",
       "      (8): Conv2d(264, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): BatchNorm2d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): LeakyReLU(negative_slope=0.8)\n",
       "      (11): Conv2d(512, 1, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (downsampling): AvgPool2d(kernel_size=3, stride=1, padding=[1, 1])\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(\"Using CUDA\" if cuda else \"Not using CUDA\")\n",
    "\n",
    "mae_loss= torch.nn.L1Loss()\n",
    "input_shape=(hp.channels,hp.img_size,hp.img_size)\n",
    "\n",
    "generator=Generator(hp.latent_dim,input_shape)\n",
    "encoder = Encoder(hp.latent_dim,input_shape)\n",
    "\n",
    "D_VAE = MultiDiscriminator(input_shape)\n",
    "D_LR = MultiDiscriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    encoder.cuda()\n",
    "    D_VAE.cuda()\n",
    "    D_LR.cuda()\n",
    "    mae_loss.cuda()\n",
    "    \n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)  # Initialize convolutional layer weights\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)  # Initialize batch normalization weights\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    \n",
    "generator.apply(weights_init_normal)\n",
    "D_VAE.apply(weights_init_normal)\n",
    "D_LR.apply(weights_init_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ae360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(batches_done):\n",
    "    generator.eval()\n",
    "    img = next(iter(val_dataloader))\n",
    "    image_samples = None\n",
    "    path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\GANs\\\\facades\\\\%s\\\\%s.png\" % ('maps', batches_done)\n",
    "    for img_A , img_B in zip(img[\"A\"],img[\"B\"]):\n",
    "        real_A = img_A.view(1,*img_A.shape).repeat(hp.latent_dim,1,1,1)\n",
    "        real_A = Variable(real_A.type(Tensor))\n",
    "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (hp.latent_dim, hp.latent_dim))))\n",
    "        fake_B = generator(real_A, sampled_z)\n",
    "        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
