{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b055a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import math\n",
    "import itertools\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053139f7",
   "metadata": {},
   "source": [
    "# Generator class\n",
    "the generator class takes the input vector(noise vector), number of color channels and number of feature maps to scale it to a image using ConvTranspose2d layers, each layer decreases the depth of feature maps while increasing the resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee399806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_ch, noise_vector, num_gen_filter):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=noise_vector,\n",
    "                out_channels=num_gen_filter * 4,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_gen_filter * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=num_gen_filter * 4,\n",
    "                out_channels=num_gen_filter * 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_gen_filter * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=num_gen_filter * 2,\n",
    "                out_channels=num_gen_filter,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_gen_filter),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=num_gen_filter,\n",
    "                out_channels=num_ch,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1093baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_ch, num_disc_filter):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_ch,\n",
    "                out_channels=num_disc_filter,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_disc_filter,\n",
    "                out_channels=num_disc_filter * 2,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_disc_filter * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_disc_filter * 2,\n",
    "                out_channels=num_disc_filter * 4,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_disc_filter * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_disc_filter * 4,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    \n",
    "    def __init__(self,noise):\n",
    "        self.input = noise\n",
    "        self.G = Generator(self.input)\n",
    "        self.D = Discriminator()\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.D.to(self.device)\n",
    "        self.G.to(self.device)\n",
    "        self.D.apply(self.weights_init)\n",
    "        self.G.apply(self.weights_init)\n",
    "        \n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "            \n",
    "    def dis_loss(self,dis_output):\n",
    "        batch_size = dis_output.size(0)\n",
    "        labels = torch.ones(batch_size).to(self.device)*0.8\n",
    "        cost = nn.BCELoss()\n",
    "        return cost(dis_output.squeeze(),labels)\n",
    "    \n",
    "    def gen_loss(self,dis_output):\n",
    "        batch_size = dis_output.size(0)\n",
    "        labels = torch.ones(batch_size).to(self.device)*0.1\n",
    "        cost = nn.BCELoss()\n",
    "        return cost(dis_output.squeeze(),labels)\n",
    "    \n",
    "    def noise(self,batch_size):\n",
    "        return torch.randn(batch_size,self.input_size,1,1).to(self.device)\n",
    "    \n",
    "    def train_generator(self,batch_size,gen_optimizer):\n",
    "        gen_optimizer.zero_grad()\n",
    "        \n",
    "        noise = self.noise(batch_size)\n",
    "        fake_images = self.G(noise)\n",
    "        dis_output = self.D(fake_images)\n",
    "        \n",
    "        gen_loss = self.gen_loss(dis_output)\n",
    "        \n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        return gen_loss.item()\n",
    "    \n",
    "    def train_discriminator(self,batch_size,dis_optimizer,real_images):\n",
    "        dis_optimizer.zero_grad()\n",
    "        \n",
    "        real_loss = self.dis_loss(self.D(real_images))\n",
    "        \n",
    "        noise = self.noise(batch_size)\n",
    "        fake_images = self.G(noise)\n",
    "        dis_output = self.D(fake_images)\n",
    "        \n",
    "        fake_loss = self.dis_loss(dis_output)\n",
    "        \n",
    "        dis_loss = real_loss + fake_loss\n",
    "        \n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        return dis_loss.item()\n",
    "    \n",
    "    def show(self, tensor, num=25, wandbactive=0, name=''):\n",
    "        data = tensor.detach().cpu()\n",
    "        fig,axes = plt.subplots(figsize=(5, 5), nrows=5, ncols=5, sharey=True, sharex=True)\n",
    "        fig.frameon = False\n",
    "        plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "        for ax, img in zip(axes.flatten(), data):\n",
    "            _, w, h = img.size()\n",
    "\n",
    "            img = img.detach().cpu().numpy()\n",
    "\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "            img = ((img + 1) * 255 / (2)).astype(np.uint8)\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "            im = ax.imshow(img.reshape((w, h, 3)))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def show_image_sample(self, noise):\n",
    "        self.G.eval()\n",
    "        self.show(self.G(noise))\n",
    "        self.G.train()\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.G.state_dict(), '/kaggle/working/Generator.pth')\n",
    "        torch.save(self.D.state_dict(), '/kaggle/working/Discriminator.pth')\n",
    "\n",
    "    def plot_weight_distribution(self, model, model_name):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.suptitle(f'Weight Distribution for {model_name}', fontsize=16, y=1.05)\n",
    "       \n",
    "        cns = 1\n",
    "        for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "            if \"weight\" in name: \n",
    "                plt.subplot(3, 3, cns)\n",
    "                plt.hist(param.detach().cpu().numpy().flatten(), bins=50)\n",
    "                plt.title(f\"Weight Distribution for {name}\")\n",
    "                plt.xlabel(\"Weight Value\")\n",
    "                plt.ylabel(\"Frequency\")\n",
    "                cns += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "\n",
    "    def train(self, batch_size, epochs, gen_optimizer, dis_optimizer, dataloader):\n",
    "        samples, losses = [], []\n",
    "        gen_loss, dis_loss, min_gen_loss = np.Inf, np.Inf, np.Inf\n",
    "        for epoch in range(epochs):\n",
    "            real_images_iter = iter(dataloader)\n",
    "            gen_loss_total, dis_loss_total, samples_count = 0, 0, 0\n",
    "            for real_images, _ in iter(real_images_iter):\n",
    "                real_images = real_images.to(self.device)\n",
    "                dis_loss = self.train_discriminator(batch_size, dis_optimizer, real_images)\n",
    "                gen_loss = self.train_generator(batch_size, gen_optimizer)\n",
    "\n",
    "            if gen_loss < min_gen_loss and epoch >= 165:\n",
    "                min_gen_loss = gen_loss\n",
    "                self.save_model()\n",
    "\n",
    "            losses.append((dis_loss, gen_loss))\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "                  f\"Discriminator Loss: {dis_loss:.4f}, Generator Loss: {gen_loss:.4f}\")\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.show_image_sample(self.noise(batch_size))\n",
    "\n",
    "        self.plot_weight_distribution(self.G, \"Generator\")\n",
    "        self.plot_weight_distribution(self.D, \"Discriminator\")\n",
    "\n",
    "        return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efda6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
